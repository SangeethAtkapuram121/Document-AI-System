{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# For handling images and data\nimport os\nimport json\nfrom PIL import Image\nimport numpy as np\n\n\n# For working with Hugging Face transformers\nfrom transformers import DonutProcessor, VisionEncoderDecoderModel\n\n# For managing and splitting data\nfrom sklearn.model_selection import train_test_split\n\n# For PyTorch (if needed for model fine-tuning)\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n# Additional utilities\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets\n\nfrom datasets import load_dataset\n\nfrom datasets import load_from_disk\n\n# Load the dataset\ndataset = load_dataset('naver-clova-ix/cord-v2')\n\n# Save the dataset locally\n# dataset.save_to_disk('./cord_v2')\n\n# dataset = load_from_disk('./cord_v2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the dataset structure\nprint(dataset)\n\n# Access the training split\ntrain_dataset = dataset['train']\n\n# Display a sample\nprint(train_dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Further you can use anyne of the either cells below for the model training","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import DonutProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n\n# Load the Donut processor (which includes the tokenizer and image processor)\nprocessor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n\n# Define the maximum sequence length for padding and truncation\nmax_length = 512  # You can adjust this value based on your dataset and model\n\n# Define the preprocessing function\ndef preprocess_data(example):\n    try:\n        # Access the image directly from the dataset (assuming it's already a PIL image)\n        image = example['image']\n\n        # Process the image using the DonutProcessor\n        pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n        # Process the annotations (text) using the DonutProcessor's tokenizer\n        target_text = processor.tokenizer(\n            example['ground_truth'],\n            add_special_tokens=True,\n            return_tensors=\"pt\",\n            padding='max_length',\n            truncation=True,\n            max_length=512\n        ).input_ids\n\n        # Ensure that both pixel_values and input_ids are valid\n        if target_text.size(1) == 0 or pixel_values is None:\n            return None\n\n        return {\"pixel_values\": pixel_values.squeeze(), \"input_ids\": target_text.squeeze()}\n    except Exception as e:\n        print(f\"Error processing sample: {example}, error: {e}\")\n        return None\n\n# Apply the preprocessing function to the dataset using batched operations\ntrain_dataset = dataset['train'].map(preprocess_data, batched=True, batch_size=8, remove_columns=dataset['train'].column_names)\nvalidation_dataset = dataset['validation'].map(preprocess_data, batched=True, batch_size=8, remove_columns=dataset['validation'].column_names)\n\n# Convert the processed datasets to PyTorch tensors\ntrain_dataset.set_format(type='torch', columns=['pixel_values', 'input_ids'])\nvalidation_dataset.set_format(type='torch', columns=['pixel_values', 'input_ids'])\n\n# Set up DataLoader for batch processing\nbatch_size = 2  # Use a smaller batch size to avoid out-of-memory errors\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, pin_memory=True)\n\n# Load the Donut model using VisionEncoderDecoderModel\nmodel = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n\n# Define a custom data collator\nclass CustomDataCollator:\n    def __call__(self, batch):\n        # Filter out any entries that might be None or missing required keys\n        batch = [item for item in batch if item and 'pixel_values' in item and 'input_ids' in item]\n\n        # If the batch is empty, raise an error or handle it\n        if not batch:\n            raise ValueError(\"Batch is empty after filtering\")\n\n        # Separate pixel values and input IDs\n        pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n        input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n\n        # Return a dictionary of batched data\n        return {\"pixel_values\": pixel_values, \"labels\": input_ids}\n\n\ndata_collator = CustomDataCollator()\n\n# Define the training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./donut_model\",\n    num_train_epochs=5,\n    per_device_train_batch_size=batch_size,  # Use the same batch size as defined earlier\n    per_device_eval_batch_size=batch_size,\n    gradient_accumulation_steps=4,  # Accumulate gradients for more effective batch size\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=3,  # Keep only the last 3 checkpoints\n    fp16=True,  # Mixed precision training\n    dataloader_pin_memory=True,\n)\n\n# Clear CUDA cache before training\ntorch.cuda.empty_cache()\n\n# Initialize the Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    data_collator=data_collator,\n    tokenizer=processor.tokenizer,\n)\n\n# Start the training process\ntrainer.train()\n\n# Save the final model\ntrainer.save_model(\"./donut_model_final\")\nprocessor.tokenizer.save_pretrained(\"./donut_model_final\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments, DonutProcessor\nimport pandas as pd\nfrom PIL import Image\n\n# Custom Dataset Class\nclass CustomDataset(Dataset):\n    def __init__(self, df, processor):\n        self.df = df\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # Get the image path and text from the dataframe\n        image_path = self.df.iloc[idx, 0]\n        text = self.df.iloc[idx, 1]\n\n        # Load the image\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # Process the image\n        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.squeeze()\n\n        # Encode the text\n        input_ids = self.processor.tokenizer(text, add_special_tokens=False, return_tensors=\"pt\").input_ids.squeeze()\n\n        # Return the processed image and encoded text\n        return {\n            \"pixel_values\": pixel_values,\n            \"input_ids\": input_ids,\n        }\n\n# Custom Data Collator to filter empty batches\nclass CustomDataCollator:\n    def __call__(self, batch):\n        # Filter out any entries that might be None or missing required keys\n        batch = [item for item in batch if item and 'pixel_values' in item and 'input_ids' in item]\n\n        # If the batch is empty, skip this batch\n        if not batch:\n            return None\n\n        # Separate pixel values and input IDs\n        pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n        input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n\n        # Return a dictionary of batched data\n        return {\"pixel_values\": pixel_values, \"labels\": input_ids}\n\n# Load your dataset\ndf = pd.DataFrame({dataset})\n\n# Load the Donut processor\nprocessor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n\n# Initialize the custom dataset\ndataset = CustomDataset(df, processor)\n\n# Create a DataLoader with the custom data collator\ndata_collator = CustomDataCollator()\ndata_loader = DataLoader(dataset, batch_size=2, collate_fn=data_collator)\n\n# Load the Donut model\nmodel = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./donut_model\",\n    per_device_train_batch_size=2,\n    num_train_epochs=3,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)\n\n# Define a custom Trainer class that skips None batches\nclass CustomTrainer(Seq2SeqTrainer):\n    def get_train_dataloader(self):\n        return data_loader\n\n    def training_step(self, model, inputs):\n        if inputs is None:\n            return None  # Skip empty batches\n\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n\n        return loss\n\n# Initialize the trainer\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    data_collator=data_collator,\n    tokenizer=processor.tokenizer,\n)\n\n# Start the training process\ntrainer.train()\n\n# Save the final model\ntrainer.save_model(\"./donut_model_final\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}